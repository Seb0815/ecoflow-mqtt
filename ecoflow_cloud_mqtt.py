#!/usr/bin/env python3
"""
EcoFlow Cloud MQTT Publisher - Standalone Version
Publiziert EcoFlow Ger√§tedaten √ºber MQTT ohne Home Assistant Abh√§ngigkeiten
"""

import asyncio
import json
import logging
import os
import signal
import struct
import sys
import threading
import time
import traceback
import datetime
from typing import Dict  # Kept for compatibility

import paho.mqtt.client as mqtt

# EcoFlow API Imports
from custom_components.ecoflow_cloud.api.private_api import EcoflowPrivateApiClient
from custom_components.ecoflow_cloud.api.public_api import EcoflowPublicApiClient
from custom_components.ecoflow_cloud.device_data import DeviceData, DeviceOptions

# Logging Setup
logging.basicConfig(
    level=logging.DEBUG,  # Aktiviere DEBUG-Logging
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
_LOGGER = logging.getLogger(__name__)


class EcoflowMqttPublisher:
    """EcoFlow MQTT Publisher mit Device-Klassen Integration"""
    
    def __init__(self):
        self.api_client = None
        self.mqtt_client = None
        self.running = False
        self.devices = {}  # Device-Instanzen f√ºr jede Seriennummer
        self.defined_parameters_cache = {}  # Cache f√ºr definierte Parameter pro Device-Typ
        
        # Konfiguration aus Umgebungsvariablen laden
        self.load_config()

    def load_config(self):
        """L√§dt Konfiguration aus Umgebungsvariablen"""
        # EcoFlow API Konfiguration (verwende immer private API)
        self.username = os.getenv("ECOFLOW_USERNAME")
        self.password = os.getenv("ECOFLOW_PASSWORD")
        
        if not self.username or not self.password:
            raise ValueError("ECOFLOW_USERNAME und ECOFLOW_PASSWORD m√ºssen gesetzt sein")
        
        # Device Liste (kommagetrennt)
        self.device_sns = [sn.strip() for sn in os.getenv("ECOFLOW_DEVICES", "").split(",") if sn.strip()]
        
        # MQTT Konfiguration
        self.mqtt_host = os.getenv("MQTT_HOST", "localhost")
        self.mqtt_port = int(os.getenv("MQTT_PORT", "1883"))
        self.mqtt_username = os.getenv("MQTT_USERNAME")
        self.mqtt_password = os.getenv("MQTT_PASSWORD")
        self.mqtt_base_topic = os.getenv("MQTT_BASE_TOPIC", "ecoflow")

    async def setup_api_client(self):
        """Erstellt EcoFlow API Client"""
        self.api_client = EcoflowPrivateApiClient(
            "api.ecoflow.com",
            self.username,
            self.password,
            "v1"
        )
        
        # Seriennummern an API Client weitergeben f√ºr MQTT Topics
        self.api_client.device_sns = self.device_sns
        
        # Login durchf√ºhren
        await self.api_client.login()
        _LOGGER.info("Successfully logged in to EcoFlow API")
        
        # Device-Instanzen erstellen basierend auf den Seriennummern
        await self.setup_device_instances()
        
        # MQTT Client starten (Python 3.13 kompatibel)
        loop = asyncio.get_running_loop()
        await loop.run_in_executor(None, self.api_client.start)
        _LOGGER.info("EcoFlow MQTT Client started")

    async def setup_device_instances(self):
        """Erstellt Device-Instanzen f√ºr alle konfigurierten Ger√§te"""
        from custom_components.ecoflow_cloud.devices import EcoflowDeviceInfo, BaseDevice
        from custom_components.ecoflow_cloud.device_data import DeviceData, DeviceOptions
        
        for device_sn in self.device_sns:
            try:
                # Automatische Ger√§teerkennung
                device_type = self.detect_device_type(device_sn)
                
                # Device-Klasse importieren
                device_class = self.get_device_class(device_type)
                
                # DeviceInfo erstellen
                device_info = EcoflowDeviceInfo(
                    public_api=False,  # Nutzen private API
                    sn=device_sn,
                    name=f"EcoFlow {device_type}",
                    device_type=device_type,
                    status=1,
                    data_topic=f"/app/device/property/{device_sn}",
                    set_topic=f"/app/device/property/{device_sn}/set",
                    set_reply_topic=f"/app/device/property/{device_sn}/set_reply",
                    get_topic=f"/app/device/property/{device_sn}/get",
                    get_reply_topic=f"/app/device/property/{device_sn}/get_reply",
                    status_topic=f"/app/device/status/{device_sn}"
                )
                
                # DeviceData mit Optionen erstellen
                device_options = DeviceOptions(
                    refresh_period=30,
                    power_step=100,
                    diagnostic_mode=False
                )
                device_data = DeviceData(
                    sn=device_sn,
                    name=f"EcoFlow {device_type}",
                    device_type=device_type,
                    options=device_options,
                    display_name=f"EcoFlow {device_type} ({device_sn})",
                    parent=None
                )
                
                # Device-Instanz erstellen
                device_instance = device_class(device_info, device_data)
                self.devices[device_sn] = device_instance
                
                # Lade definierte Parameter f√ºr Filterung
                defined_params = self.get_defined_parameters_for_device(device_instance, device_type)
                _LOGGER.info(f"Device {device_type} ({device_sn}): {len(defined_params)} defined parameters will be published via MQTT")
                
                _LOGGER.info(f"Created {device_type} device instance for {device_sn}")
                
            except Exception as e:
                _LOGGER.error(f"Failed to create device instance for {device_sn}: {e}")
                _LOGGER.error(f"Detailed error: {traceback.format_exc()}")
                # Fallback: Nutze DiagnosticDevice
                from custom_components.ecoflow_cloud.devices import DiagnosticDevice
                self.devices[device_sn] = None  # Placeholder
        
        _LOGGER.info(f"Created {len([d for d in self.devices.values() if d is not None])} device instances")

    def get_defined_parameters_for_device(self, device_instance, device_type: str) -> set:
        """Extrahiert alle in der Device-Klasse definierten Parameter durch direkte Analyse der Device-Klasse"""
        
        # Cache pr√ºfen - Parameter nur einmal pro Device-Typ extrahieren
        if device_type in self.defined_parameters_cache:
            _LOGGER.debug(f"Using cached parameters for {device_type}: {len(self.defined_parameters_cache[device_type])} parameters")
            return self.defined_parameters_cache[device_type]
        
        defined_params = set()
        
        try:
            # Verwende die neue get_defined_parameters Methode der Device-Klasse falls verf√ºgbar
            if hasattr(device_instance, 'get_defined_parameters') and callable(device_instance.get_defined_parameters):
                defined_params = device_instance.get_defined_parameters()
                _LOGGER.info(f"Used device class method: {len(defined_params)} parameters for {device_type}")
            
            # Fallback: Extrahiere Parameter aus sensors() Methode
            elif hasattr(device_instance, 'sensors') and callable(device_instance.sensors):
                try:
                    sensors = device_instance.sensors(None)  # Dummy client
                    
                    for sensor in sensors:
                        # Der dritte Parameter im Konstruktor ist der Parameter-Name
                        if hasattr(sensor, 'attr_key') and sensor.attr_key:
                            defined_params.add(sensor.attr_key)
                        elif hasattr(sensor, '_attr_key') and sensor._attr_key:
                            defined_params.add(sensor._attr_key)
                    
                    _LOGGER.info(f"Dynamically extracted {len(defined_params)} parameters for {device_type}")
                    
                except Exception as e:
                    _LOGGER.debug(f"Dynamic parameter extraction failed for {device_type}: {e}")
            
            # Letzter Fallback: Lade Device-Klasse direkt f√ºr Parameter-Extraktion
            else:
                _LOGGER.info(f"Loading device class for parameter extraction: {device_type}")
                device_class = self.get_device_class(device_type)
                if device_class and hasattr(device_class, 'get_defined_parameters'):
                    defined_params = device_class.get_defined_parameters()
                    _LOGGER.info(f"Used direct device class: {len(defined_params)} parameters for {device_type}")
                    
        except Exception as e:
            _LOGGER.error(f"Error getting defined parameters for {device_type}: {e}")
            _LOGGER.error(f"Detailed error: {traceback.format_exc()}")
            
        # Wenn nichts gefunden wurde, gebe leeres Set zur√ºck
        if not defined_params:
            _LOGGER.warning(f"No defined parameters found for {device_type} - all parameters will be logged only")
        
        # Cache die Ergebnisse
        self.defined_parameters_cache[device_type] = defined_params
        _LOGGER.debug(f"Cached {len(defined_params)} parameters for {device_type}")
            
        return defined_params

    def get_device_class(self, device_type: str):
        """L√§dt die entsprechende Device-Klasse basierend auf dem Ger√§tetyp"""
        device_map = {
            "DELTA_2": "delta2.Delta2",
            "DELTA_2_MAX": "delta2_max.Delta2Max", 
            "DELTA_PRO": "delta_pro.DeltaPro",
            "DELTA_MAX": "delta_max.DeltaMax",
            "DELTA_MINI": "delta_mini.DeltaMini",
            "RIVER_2": "river2.River2",
            "RIVER_2_MAX": "river2_max.River2Max",
            "RIVER_2_PRO": "river2_pro.River2Pro", 
            "RIVER_MAX": "river_max.RiverMax",
            "RIVER_MINI": "river_mini.RiverMini",
            "RIVER_PRO": "river_pro.RiverPro",
            "POWERSTREAM": "powerstream.PowerStream",
            "STREAM_AC": "stream_ac.StreamAC",
            "STREAM_ULTRA": "stream_ac.StreamAC",  # Stream Ultra nutzt StreamAC Klasse
            "STREAM_PRO": "stream_ac.StreamAC",
            "GLACIER": "glacier.Glacier",
            "WAVE_2": "wave2.Wave2",
            "SMART_METER": "smart_meter.SmartMeter",
        }
        
        if device_type not in device_map:
            _LOGGER.warning(f"Unknown device type: {device_type}, using DiagnosticDevice")
            from custom_components.ecoflow_cloud.devices import DiagnosticDevice
            return DiagnosticDevice
        
        # Dynamischer Import der Device-Klasse
        module_path, class_name = device_map[device_type].rsplit('.', 1)
        try:
            module = __import__(f"custom_components.ecoflow_cloud.devices.internal.{module_path}", fromlist=[class_name])
            device_class = getattr(module, class_name)
            _LOGGER.info(f"Successfully loaded device class: {device_type} -> {class_name}")
            return device_class
        except Exception as e:
            _LOGGER.error(f"Failed to load {device_type} device class: {e}")
            _LOGGER.error(f"Detailed error: {traceback.format_exc()}")
            _LOGGER.warning(f"Using DiagnosticDevice as fallback for {device_type}")
            from custom_components.ecoflow_cloud.devices import DiagnosticDevice
            return DiagnosticDevice

    def setup_mqtt_client(self):
        """Konfiguriert den lokalen MQTT Client"""
        self.mqtt_client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION1)
        
        if self.mqtt_username and self.mqtt_password:
            self.mqtt_client.username_pw_set(self.mqtt_username, self.mqtt_password)
            
        self.mqtt_client.on_connect = self.on_mqtt_connect
        self.mqtt_client.on_disconnect = self.on_mqtt_disconnect
        
        try:
            self.mqtt_client.connect(self.mqtt_host, self.mqtt_port, 60)
            self.mqtt_client.loop_start()
            _LOGGER.info(f"Connected to local MQTT broker: {self.mqtt_host}:{self.mqtt_port}")
        except Exception as e:
            _LOGGER.error(f"Error connecting to MQTT broker: {e}")
            raise

    def on_mqtt_connect(self, client, userdata, flags, rc):
        """Callback f√ºr MQTT Verbindung"""
        if rc == 0:
            _LOGGER.info("Successfully connected to local MQTT broker")
        else:
            _LOGGER.error(f"MQTT connection failed with code: {rc}")

    def on_mqtt_disconnect(self, client, userdata, rc):
        """Callback f√ºr MQTT Trennung"""
        _LOGGER.warning(f"MQTT connection disconnected with code: {rc}")

    def setup_ecoflow_message_forwarding(self):
        """Setup Message Forwarding vom EcoFlow MQTT zum lokalen Broker"""
        if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
            # Zugriff auf den inneren paho-mqtt Client
            ecoflow_paho_client = None
            if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                ecoflow_paho_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
            elif hasattr(self.api_client.mqtt_client, '__client'):
                ecoflow_paho_client = self.api_client.mqtt_client.__client
            
            if ecoflow_paho_client:
                # Original-Callback sichern
                original_on_message = ecoflow_paho_client.on_message
                original_on_connect = ecoflow_paho_client.on_connect
                
                # Verbindungs√ºberwachung hinzuf√ºgen
                def connection_wrapper(client, userdata, flags, rc):
                    if rc == 0:
                        _LOGGER.info("EcoFlow MQTT broker connection successfully established")
                    else:
                        _LOGGER.warning(f"EcoFlow MQTT connection failed: RC={rc}")
                    
                    # Original-Callback ausf√ºhren
                    if original_on_connect:
                        original_on_connect(client, userdata, flags, rc)
                
                # Wrapper-Callback erstellen
                def message_wrapper(client, userdata, message):
                    # Original-Callback ausf√ºhren
                    if original_on_message:
                        original_on_message(client, userdata, message)
                    
                    # Unsere eigene Verarbeitung
                    self.on_ecoflow_message(client, userdata, message)
                
                # Wrapper setzen
                ecoflow_paho_client.on_message = message_wrapper
                ecoflow_paho_client.on_connect = connection_wrapper
                _LOGGER.info("EcoFlow MQTT message handler successfully set")
                
                # Debug: MQTT-Client Status √ºberpr√ºfen (initial)
                _LOGGER.info(f"EcoFlow MQTT Client initial status: Connected={ecoflow_paho_client.is_connected()}")
                if hasattr(ecoflow_paho_client, '_host'):
                    _LOGGER.info(f"EcoFlow MQTT Client host: {ecoflow_paho_client._host}")
                
                # Timeout-basierte Reconnection setup (wie ioBroker)
                self.setup_message_timeout_monitoring()
                
            else:
                _LOGGER.warning("Could not access EcoFlow paho-mqtt client")
        else:
            _LOGGER.warning("EcoFlow MQTT client not available")

    def setup_message_timeout_monitoring(self):
        """
        Setup timeout-based monitoring wie in ioBroker implementation
        Automatische Reconnection wenn keine Nachrichten empfangen werden
        """
        self.last_message_time = time.time()
        self.message_timeout_timer = None
        self.message_timeout_interval = 600  # 10 Minuten wie in ioBroker
        
        # Starte initial timeout
        self.reset_message_timeout()
        
    def reset_message_timeout(self):
        """Reset message timeout timer nach jeder empfangenen Nachricht"""
        try:
            # Alten Timer stoppen
            if hasattr(self, 'message_timeout_timer') and self.message_timeout_timer:
                self.message_timeout_timer.cancel()
                
            # Neuen Timer starten
            import threading
            self.message_timeout_timer = threading.Timer(
                self.message_timeout_interval, 
                self.handle_message_timeout
            )
            self.message_timeout_timer.start()
            
            _LOGGER.debug(f"Message timeout reset - will trigger in {self.message_timeout_interval}s")
            
        except Exception as e:
            _LOGGER.debug(f"Error resetting message timeout: {e}")
            
    def handle_message_timeout(self):
        """
        Handle message timeout - f√ºhrt Reconnection durch
        Wird aufgerufen wenn keine Nachrichten in timeout_interval empfangen wurden
        """
        try:
            _LOGGER.warning(f"üö® No messages received in {self.message_timeout_interval}s - triggering reconnection")
            
            # Reconnection counter erh√∂hen
            if not hasattr(self, 'reconnection_count'):
                self.reconnection_count = 0
            self.reconnection_count += 1
            
            # MQTT Reconnection versuchen
            self.trigger_mqtt_reconnection()
            
            # Statistik publizieren
            if self.mqtt_client:
                reconnection_status = {
                    'reconnection_triggered': True,
                    'reconnection_count': self.reconnection_count,
                    'last_message_time': self.last_message_time,
                    'timeout_interval': self.message_timeout_interval,
                    'timestamp': time.time()
                }
                
                self.mqtt_client.publish(
                    f"{self.mqtt_base_topic}/reconnection_status", 
                    json.dumps(reconnection_status), 
                    retain=True
                )
            
            # Timer f√ºr n√§chsten Check wieder starten
            self.reset_message_timeout()
            
        except Exception as e:
            _LOGGER.error(f"Error handling message timeout: {e}")
            
    def trigger_mqtt_reconnection(self):
        """
        F√ºhrt MQTT Reconnection durch (wie ioBroker implementation)
        """
        try:
            if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
                mqtt_client = None
                
                # MQTT Client finden
                if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                    mqtt_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
                elif hasattr(self.api_client.mqtt_client, '__client'):
                    mqtt_client = self.api_client.mqtt_client.__client
                
                if mqtt_client:
                    _LOGGER.info("Triggering MQTT reconnection...")
                    
                    # Disconnect und reconnect
                    try:
                        mqtt_client.disconnect()
                        time.sleep(2)  # Kurz warten
                        mqtt_client.reconnect()
                        _LOGGER.info("‚úÖ MQTT reconnection initiated")
                    except Exception as reconnect_error:
                        _LOGGER.error(f"‚ùå MQTT reconnection failed: {reconnect_error}")
                        
                        # Fallback: Komplette API Client reconnection
                        try:
                            _LOGGER.info("Trying complete API client reconnection...")
                            if hasattr(self.api_client, 'stop'):
                                self.api_client.stop()
                            time.sleep(5)
                            # API Client w√ºrde hier neu initialisiert werden
                            # Das m√ºsste in setup_api_client implementiert werden
                            _LOGGER.info("‚úÖ Complete reconnection attempt finished")
                        except Exception as full_reconnect_error:
                            _LOGGER.error(f"‚ùå Complete reconnection failed: {full_reconnect_error}")
                else:
                    _LOGGER.error("‚ùå Could not find MQTT client for reconnection")
            else:
                _LOGGER.error("‚ùå EcoFlow MQTT client not available for reconnection")
                
        except Exception as e:
            _LOGGER.error(f"‚ùå Error triggering MQTT reconnection: {e}")

    def on_ecoflow_message(self, client, userdata, message):
        """Callback f√ºr EcoFlow MQTT-Nachrichten - nutzt Device-Klassen f√ºr Dekodierung"""
        try:
            # Nachrichten-Statistik aktualisieren
            self.message_count = getattr(self, 'message_count', 0) + 1
            self.last_message_time = time.time()
            
            # Message timeout timer zur√ºcksetzen (wie in ioBroker)
            if hasattr(self, 'reset_message_timeout'):
                self.reset_message_timeout()
            
            topic = message.topic
            payload = message.payload
            
            _LOGGER.info(f"EcoFlow MQTT message #{self.message_count} received - Topic: {topic}, Payload size: {len(payload)} bytes")
            
            # Versuche Ger√§te-SN zu extrahieren
            device_sn = None
            for sn in self.device_sns:
                if sn in topic:
                    device_sn = sn
                    break
            
            if not device_sn:
                _LOGGER.warning(f"No device SN found in topic: {topic}")
                return
            
            # Device-Instanz holen
            device_instance = self.devices.get(device_sn)
            if not device_instance:
                _LOGGER.warning(f"No device instance found for {device_sn}")
                return
            
            device_type = self.detect_device_type(device_sn)
            _LOGGER.info(f"Processing message for {device_type} ({device_sn}) - Topic: {topic}")
            
            # Device-spezifische Dekodierung mit der echten Device-Klasse
            decoded_data = self.decode_with_device_class(device_instance, device_sn, device_type, payload)
            
            if decoded_data and "params" in decoded_data and decoded_data["params"]:
                param_count = len(decoded_data["params"])
                _LOGGER.info(f"Device class decoded {device_type} data: {param_count} parameters")
                
                # Detaillierte Parameter-Info f√ºr wichtige Werte
                important_params = self.extract_important_parameters(decoded_data["params"])
                if important_params:
                    _LOGGER.info(f"Key parameters: {', '.join(important_params)}")
                
                # JSON f√ºr MQTT erstellen
                mqtt_data = {
                    "device_sn": device_sn,
                    "device_type": device_type,
                    "timestamp": time.time(),
                    "decoding_method": "device_class",
                    "message_count": self.message_count,
                    "params": decoded_data["params"]
                }
                
                # Rohdaten optional hinzuf√ºgen (f√ºr Debugging)
                if len(payload) < 1000:  # Nur bei kleinen Payloads
                    mqtt_data["raw_hex"] = payload.hex()
                    mqtt_data["raw_bytes"] = len(payload)
                
                payload_str = json.dumps(mqtt_data, indent=2, ensure_ascii=False, default=str)
                
            else:
                # Fallback: Als hex-string mit Ger√§tetyp
                _LOGGER.warning(f"Device class decoding failed for {device_type}, using hex fallback")
                mqtt_data = {
                    "device_sn": device_sn,
                    "device_type": device_type,
                    "timestamp": time.time(),
                    "decoding_method": "hex_fallback",
                    "message_count": self.message_count,
                    "raw_hex": payload.hex(),
                    "raw_bytes": len(payload),
                    "params": {}
                }
                payload_str = json.dumps(mqtt_data, indent=2)
            
            # Weiterleitung an lokalen MQTT Broker
            local_topic = f"{self.mqtt_base_topic}/{device_sn}/data"
            self.mqtt_client.publish(local_topic, payload_str, retain=True)
            
            # Ger√§te-spezifisches Topic
            device_topic = f"{self.mqtt_base_topic}/{device_type.lower()}/{device_sn}/data"
            self.mqtt_client.publish(device_topic, payload_str, retain=True)
            
            # Parameter-spezifische Topics nur f√ºr definierte Parameter
            if "params" in mqtt_data and mqtt_data["params"]:
                self.publish_filtered_parameters(device_instance, device_sn, device_type, mqtt_data["params"], mqtt_data.get("timestamp", time.time()))
            
            # Auch Original-Topic Structure beibehalten
            clean_topic = topic.replace("/app/", "").replace("+", "unknown")
            local_orig_topic = f"{self.mqtt_base_topic}/raw/{clean_topic}"
            self.mqtt_client.publish(local_orig_topic, payload_str, retain=True)
            
            _LOGGER.info(f"EcoFlow {device_type} data forwarded: {topic} -> {local_topic}")
                
        except Exception as e:
            _LOGGER.error(f"Error forwarding EcoFlow message: {e}")
            import traceback
            _LOGGER.error(traceback.format_exc())

    def decode_with_device_class(self, device_instance, device_sn: str, device_type: str, payload: bytes) -> dict:
        """Nutzt die echte Device-Klasse f√ºr die Protobuf-Dekodierung - generisch f√ºr alle Ger√§tetypen"""
        try:
            # Basis-Struktur
            result = {
                "device_sn": device_sn,
                "device_type": device_type,
                "timestamp": time.time(),
                "params": {}
            }
            
            # Device-Klasse f√ºr Dekodierung nutzen
            _LOGGER.debug(f"Device instance type: {type(device_instance)}")
            _LOGGER.debug(f"Device instance has _prepare_data: {hasattr(device_instance, '_prepare_data')}")
            
            if hasattr(device_instance, '_prepare_data'):
                # Device hat eigene _prepare_data Methode
                _LOGGER.debug(f"Using _prepare_data method for {device_type}")
                decoded_raw = device_instance._prepare_data(payload)
                
                if decoded_raw and "params" in decoded_raw:
                    result["params"] = decoded_raw["params"]
                    result["decoding_success"] = True
                    _LOGGER.info(f"Device class _prepare_data successful: {len(result['params'])} parameters")
                else:
                    _LOGGER.debug(f"Device _prepare_data returned no params: {decoded_raw}")
                    result["decoding_success"] = False
                    
            elif hasattr(device_instance, 'data') and hasattr(device_instance.data, 'update_data'):
                # Standard BaseDevice mit EcoflowDataHolder
                _LOGGER.debug(f"Using DataHolder for {device_type}")
                
                # Versuche direkte Protobuf-Dekodierung f√ºr bessere Ergebnisse
                protobuf_decoded = self.decode_device_protobuf_direct(device_type, payload)
                if protobuf_decoded and protobuf_decoded.get("params"):
                    # Nutze die direkten Protobuf-Ergebnisse
                    device_instance.data.update_data(protobuf_decoded)
                    result["params"] = protobuf_decoded["params"]
                    result["decoding_success"] = True
                    _LOGGER.info(f"Device DataHolder with protobuf successful: {len(result['params'])} parameters")
                else:
                    # Fallback: Hex-Darstellung
                    hex_data = {"params": {"raw_hex": payload.hex(), "raw_length": len(payload)}}
                    device_instance.data.update_data(hex_data)
                    
                    # Daten aus dem DataHolder extrahieren
                    if hasattr(device_instance.data, 'params') and device_instance.data.params:
                        result["params"] = dict(device_instance.data.params)
                        result["decoding_success"] = True
                        _LOGGER.info(f"Device DataHolder fallback successful: {len(result['params'])} parameters")
                    else:
                        _LOGGER.debug(f"Device DataHolder returned no data")
                        result["decoding_success"] = False
            else:
                _LOGGER.warning(f"Device instance {device_type} has no known decoding method")
                
                # Universeller Fallback: Direktes Protobuf-Dekodieren f√ºr alle Ger√§tetypen
                _LOGGER.info(f"Attempting direct protobuf decoding for {device_type}")
                fallback_decoded = self.decode_device_protobuf_direct(device_type, payload)
                if fallback_decoded and fallback_decoded.get("params"):
                    result["params"] = fallback_decoded["params"]
                    result["decoding_success"] = True
                    _LOGGER.info(f"Direct protobuf successful: {len(result['params'])} parameters")
                    return result
                
                result["decoding_success"] = False
            
            return result
            
        except Exception as e:
            _LOGGER.error(f"Device class decoding failed for {device_type}: {e}")
            _LOGGER.error(f"Full traceback: {traceback.format_exc()}")
            return {
                "device_sn": device_sn,
                "device_type": device_type,
                "timestamp": time.time(),
                "decoding_success": False,
                "error": str(e),
                "params": {}
            }

    def extract_important_parameters(self, params: dict) -> list:
        """Extrahiert wichtige Parameter f√ºr Logging"""
        important = []
        
        # Battery/SOC Parameter
        for key in ["battery_soc", "soc", "battery_percentage", "f32ShowSoc", "bmsBattSoc", "bms_bmsStatus.soc"]:
            if key in params and isinstance(params[key], (int, float)):
                important.append(f"{key}={params[key]}%")
        
        # Power Parameter
        power_keys = [k for k in params.keys() if any(term in k.lower() for term in ["power", "watt", "inputwatts", "outputwatts"])]
        for key in power_keys[:3]:  # Nur die ersten 3
            if isinstance(params[key], (int, float)):
                important.append(f"{key}={params[key]}W")
        
        # Cycle Parameter
        for key in ["cycles", "battery_cycles"]:
            if key in params and isinstance(params[key], (int, float)):
                important.append(f"{key}={params[key]}")
        
        # Temperature Parameter  
        temp_keys = [k for k in params.keys() if "temp" in k.lower()]
        for key in temp_keys[:2]:  # Nur die ersten 2
            if isinstance(params[key], (int, float)):
                important.append(f"{key}={params[key]}¬∞C")
        
        return important[:5]  # Maximal 5 wichtige Parameter

    def publish_parameter_topics(self, device_sn: str, device_type: str, params: dict):
        """Publiziert einzelne Parameter auf separaten MQTT Topics"""
        try:
            # Nur f√ºr wichtige Parameter separate Topics erstellen
            important_params = {
                "battery_soc": ["battery_soc", "soc", "battery_percentage", "f32ShowSoc"],
                "power_in": ["inputWatts", "pd.wattsInSum", "inv.inputWatts", "mppt.inWatts"],
                "power_out": ["outputWatts", "pd.wattsOutSum", "inv.outputWatts", "mppt.outWatts"],
                "battery_cycles": ["cycles", "battery_cycles"],
                "temperature": ["temp", "battery_temp", "maxCellTemp"],
                "voltage": ["voltage", "vol", "battery_voltage"]
            }
            
            for param_type, possible_keys in important_params.items():
                for key in possible_keys:
                    if key in params and isinstance(params[key], (int, float)):
                        topic = f"{self.mqtt_base_topic}/{device_sn}/{param_type}"
                        value = params[key]
                        
                        # Einfaches JSON mit Wert und Zeitstempel
                        data = {
                            "value": value,
                            "timestamp": time.time(),
                            "unit": self.get_parameter_unit(param_type)
                        }
                        
                        self.mqtt_client.publish(topic, json.dumps(data), retain=True)
                        break  # Nur den ersten gefundenen Parameter verwenden
                        
        except Exception as e:
            _LOGGER.debug(f"Error publishing parameter topics: {e}")

    def publish_filtered_parameters(self, device_instance, device_sn: str, device_type: str, params: dict, timestamp: float):
        """Publiziert nur definierte Parameter √ºber MQTT, loggt alle anderen f√ºr m√∂gliche Integration"""
        try:
            # Hole definierte Parameter f√ºr dieses Device
            defined_params = self.get_defined_parameters_for_device(device_instance, device_type)
            
            # Separiere Parameter in definierte und undefinierte
            defined_found = {}
            undefined_found = {}
            
            # Spezielle SOC-Parameter immer als definiert behandeln
            soc_patterns = [
                'soc', 'f32showsoc', 'bmsbattsoc', 'cmsbattsoc', 'battery_soc',
                'actsoc', 'targetsoc', 'maxchgsoc', 'mindchgsoc', 'diffsoc',
                'backupreversesoc', 'lcdshowsoc', 'f32lcdshowsoc'
            ]
            
            for param_name, param_value in params.items():
                param_lower = param_name.lower().replace('_', '').replace('.', '')
                
                # Check if it's a SOC parameter or already defined
                is_soc_param = any(soc_pattern in param_lower for soc_pattern in soc_patterns)
                is_defined = param_name in defined_params
                
                if is_defined or is_soc_param:
                    defined_found[param_name] = param_value
                    if is_soc_param and not is_defined:
                        _LOGGER.info(f"SOC parameter '{param_name}' auto-promoted to defined: {param_value}")
                else:
                    undefined_found[param_name] = param_value
            
            # Publiziere nur definierte Parameter √ºber MQTT
            for param_name, param_value in defined_found.items():
                if param_name in ["error", "raw_hex", "raw_length"]:
                    # Fehler-Parameter auf separates Topic
                    topic = f"{self.mqtt_base_topic}/{device_sn}/errors/{param_name}"
                else:
                    # Normale Parameter auf eigenes Topic
                    topic = f"{self.mqtt_base_topic}/{device_sn}/{param_name}"
                
                # Parameter-Wert zu JSON-kompatiblem Format konvertieren
                json_compatible_value = self.convert_to_json_compatible(param_value)
                
                # Nur den reinen Wert publizieren (ohne Metadaten)
                try:
                    # F√ºr einfache Werte: direkt als String oder Zahl publizieren
                    if isinstance(json_compatible_value, (int, float)):
                        payload = str(json_compatible_value)
                    elif isinstance(json_compatible_value, bool):
                        payload = "true" if json_compatible_value else "false"
                    elif isinstance(json_compatible_value, str):
                        payload = json_compatible_value
                    elif json_compatible_value is None:
                        payload = "null"
                    else:
                        # F√ºr komplexe Objekte: als JSON publizieren
                        payload = json.dumps(json_compatible_value, ensure_ascii=False, default=str)
                    
                    self.mqtt_client.publish(topic, payload, retain=True)
                    
                except (TypeError, ValueError) as json_error:
                    # Fallback: String-Konvertierung
                    payload = str(json_compatible_value)
                    self.mqtt_client.publish(topic, payload, retain=True)
                    _LOGGER.debug(f"Used string fallback for parameter {param_name}: {json_error}")
            
            # Logge undefinierte Parameter f√ºr m√∂gliche Integration
            if undefined_found:
                undefined_summary = []
                for param_name, param_value in undefined_found.items():
                    # Erstelle kompakte Darstellung
                    if isinstance(param_value, (int, float)):
                        undefined_summary.append(f"{param_name}={param_value}")
                    elif isinstance(param_value, bool):
                        undefined_summary.append(f"{param_name}={param_value}")
                    elif isinstance(param_value, str) and len(param_value) < 50:
                        undefined_summary.append(f"{param_name}='{param_value}'")
                    else:
                        undefined_summary.append(f"{param_name}=<{type(param_value).__name__}>")
                
                _LOGGER.info(f"UNDEFINED PARAMETERS for {device_type} ({device_sn}): {len(undefined_found)} parameters found")
                _LOGGER.info(f"UNDEFINED PARAMETERS: {', '.join(undefined_summary[:10])}" + 
                           (f" ... and {len(undefined_summary)-10} more" if len(undefined_summary) > 10 else ""))
                
            
            _LOGGER.debug(f"Published {len(defined_found)} defined parameters for {device_sn}, " +
                         f"logged {len(undefined_found)} undefined parameters")
                        
        except Exception as e:
            _LOGGER.error(f"Error in filtered parameter publishing: {e}")
            _LOGGER.error(traceback.format_exc())

    def publish_all_parameters(self, device_sn: str, device_type: str, params: dict, timestamp: float):
        """Publiziert ALLE Parameter auf separate MQTT Topics - nur der reine Wert"""
        try:
            for param_name, param_value in params.items():
                if param_name in ["error", "raw_hex", "raw_length"]:
                    # Fehler-Parameter auf separates Topic
                    topic = f"{self.mqtt_base_topic}/{device_sn}/errors/{param_name}"
                else:
                    # Normale Parameter auf eigenes Topic
                    topic = f"{self.mqtt_base_topic}/{device_sn}/{param_name}"
                
                # Parameter-Wert zu JSON-kompatiblem Format konvertieren
                json_compatible_value = self.convert_to_json_compatible(param_value)
                
                # Nur den reinen Wert publizieren (ohne Metadaten)
                try:
                    # F√ºr einfache Werte: direkt als String oder Zahl publizieren
                    if isinstance(json_compatible_value, (int, float)):
                        payload = str(json_compatible_value)
                    elif isinstance(json_compatible_value, bool):
                        payload = "true" if json_compatible_value else "false"
                    elif isinstance(json_compatible_value, str):
                        payload = json_compatible_value
                    elif json_compatible_value is None:
                        payload = "null"
                    else:
                        # F√ºr komplexe Objekte: als JSON publizieren
                        payload = json.dumps(json_compatible_value, ensure_ascii=False, default=str)
                    
                    self.mqtt_client.publish(topic, payload, retain=True)
                    
                except (TypeError, ValueError) as json_error:
                    # Fallback: String-Konvertierung
                    payload = str(json_compatible_value)
                    self.mqtt_client.publish(topic, payload, retain=True)
                    _LOGGER.debug(f"Used string fallback for parameter {param_name}: {json_error}")
                
            _LOGGER.debug(f"Published {len(params)} individual parameters for {device_sn}")
                        
        except Exception as e:
            _LOGGER.error(f"Error publishing all parameters: {e}")
            import traceback
            _LOGGER.error(traceback.format_exc())

    def convert_to_json_compatible(self, value):
        """Konvertiert beliebige Python-Objekte zu JSON-kompatiblen Formaten"""
        try:
            # Primitive Typen sind bereits JSON-kompatibel
            if value is None or isinstance(value, (bool, int, float, str)):
                return value
            
            # Listen und Tuples rekursiv konvertieren
            elif isinstance(value, (list, tuple)):
                return [self.convert_to_json_compatible(item) for item in value]
            
            # Dictionaries rekursiv konvertieren
            elif isinstance(value, dict):
                return {str(k): self.convert_to_json_compatible(v) for k, v in value.items()}
            
            # Bytes zu Hex-String
            elif isinstance(value, bytes):
                return value.hex()
            
            # Protobuf-Objekte (haben DESCRIPTOR Attribut)
            elif hasattr(value, 'DESCRIPTOR'):
                result = {}
                result["_protobuf_type"] = type(value).__name__
                
                # Alle Felder des Protobuf-Objekts extrahieren
                for field in value.DESCRIPTOR.fields:
                    try:
                        if value.HasField(field.name):
                            field_value = getattr(value, field.name)
                            result[field.name] = self.convert_to_json_compatible(field_value)
                    except Exception as field_error:
                        result[field.name] = f"<extraction_error: {field_error}>"
                
                return result
            
            # Enum-Werte
            elif hasattr(value, 'name') and hasattr(value, 'value'):
                return {
                    "_enum_type": type(value).__name__,
                    "name": value.name,
                    "value": value.value
                }
            
            # Datetime-Objekte
            elif hasattr(value, 'isoformat'):
                return value.isoformat()
            
            # Andere Objekte mit __dict__
            elif hasattr(value, '__dict__'):
                result = {
                    "_object_type": type(value).__name__,
                    "_module": getattr(type(value), '__module__', 'unknown')
                }
                # Versuche alle Attribute zu extrahieren
                for attr_name in dir(value):
                    if not attr_name.startswith('_'):
                        try:
                            attr_value = getattr(value, attr_name)
                            if not callable(attr_value):
                                result[attr_name] = self.convert_to_json_compatible(attr_value)
                        except Exception:
                            continue
                return result
            
            # Fallback: String-Konvertierung
            else:
                return {
                    "_fallback_string": str(value),
                    "_original_type": type(value).__name__
                }
                
        except Exception as e:
            # Absoluter Fallback
            return {
                "_conversion_error": str(e),
                "_original_value": str(value) if value is not None else None,
                "_original_type": type(value).__name__ if value is not None else "NoneType"
            }

    def get_parameter_unit_extended(self, param_name: str) -> str:
        """Erweiterte Einheiten-Erkennung basierend auf Parameter-Namen"""
        # Power-related parameters
        if any(keyword in param_name.lower() for keyword in ["power", "watt", "gridconnection"]):
            return "W"
        
        # SOC/Battery percentage
        if any(keyword in param_name.lower() for keyword in ["soc", "battery_percentage", "showsoc"]):
            return "%"
            
        # Voltage
        if any(keyword in param_name.lower() for keyword in ["vol", "voltage"]):
            return "V"
            
        # Current
        if any(keyword in param_name.lower() for keyword in ["current", "amp"]):
            return "A"
            
        # Temperature
        if any(keyword in param_name.lower() for keyword in ["temp", "temperature"]):
            return "¬∞C"
            
        # Capacity
        if any(keyword in param_name.lower() for keyword in ["cap", "capacity"]):
            return "Ah"
            
        # Time (minutes/seconds)
        if any(keyword in param_name.lower() for keyword in ["time", "remaining"]):
            return "min"
            
        # Cycles
        if "cycle" in param_name.lower():
            return "cycles"
            
        return ""

    def get_parameter_unit(self, param_type: str) -> str:
        """Gibt die Einheit f√ºr Parameter-Typen zur√ºck"""
        units = {
            "battery_soc": "%",
            "power_in": "W", 
            "power_out": "W",
            "battery_cycles": "cycles",
            "temperature": "¬∞C",
            "voltage": "V"
        }
        return units.get(param_type, "")

    def detect_device_type(self, device_sn: str) -> str:
        """Automatische Erkennung des Ger√§tetyps basierend auf der Seriennummer"""
        # EcoFlow Seriennummer-Patterns (bekannte Prefixe)
        sn_patterns = {
            "BK11": "STREAM_ULTRA",   # Stream Ultra
            "BK21": "STREAM_AC",      # Stream AC
            "BK31": "STREAM_PRO",     # Stream Pro
            "HW52": "POWERSTREAM",    # PowerStream
            "R331": "DELTA_2",        # Delta 2
            "R351": "DELTA_2_MAX",    # Delta 2 Max
            "R711": "RIVER_2",        # River 2
            "R750": "RIVER_2_MAX",    # River 2 Max
            "R600": "RIVER_PRO",      # River Pro
            "R210": "RIVER_MAX",      # River Max
            "R211": "RIVER_MINI",     # River Mini
        }
        
        # Pr√ºfe bekannte Patterns
        for prefix, device_type in sn_patterns.items():
            if device_sn.startswith(prefix):
                _LOGGER.info(f"Device type detected: {device_sn} -> {device_type}")
                return device_type
        
        # Fallback: Versuche aus SN-Struktur zu erraten
        if device_sn.startswith("BK"):
            _LOGGER.warning(f"Unknown Stream type for SN: {device_sn}, using STREAM_AC")
            return "STREAM_AC"
        elif device_sn.startswith("R"):
            _LOGGER.warning(f"Unknown River/Delta type for SN: {device_sn}, using DELTA_2")
            return "DELTA_2"
        elif device_sn.startswith("HW"):
            _LOGGER.warning(f"Unknown PowerStream type for SN: {device_sn}, using POWERSTREAM")
            return "POWERSTREAM"
        else:
            _LOGGER.warning(f"Completely unknown device type for SN: {device_sn}")
            return "UNKNOWN"

    def decode_device_protobuf_direct(self, device_type: str, payload: bytes) -> dict:
        """Universelle direkte Protobuf-Dekodierung als Fallback f√ºr alle Ger√§tetypen"""
        try:
            _LOGGER.debug(f"Direct protobuf decoding: {device_type} - {len(payload)} bytes")
            
            # Ger√§tespezifische Dekodierung basierend auf Ger√§tetyp
            if device_type in ["STREAM_ULTRA", "STREAM_AC", "STREAM_PRO"]:
                return self._decode_stream_protobuf(payload)
            elif device_type in ["DELTA_2", "DELTA_2_MAX", "DELTA_PRO", "DELTA_MAX", "DELTA_MINI"]:
                return self._decode_delta_protobuf(payload)
            elif device_type in ["RIVER_2", "RIVER_2_MAX", "RIVER_2_PRO", "RIVER_MAX", "RIVER_MINI", "RIVER_PRO"]:
                return self._decode_river_protobuf(payload)
            elif device_type == "POWERSTREAM":
                return self._decode_powerstream_protobuf(payload)
            elif device_type == "GLACIER":
                return self._decode_glacier_protobuf(payload)
            elif device_type == "WAVE_2":
                return self._decode_wave_protobuf(payload)
            elif device_type == "SMART_METER":
                return self._decode_smart_meter_protobuf(payload)
            else:
                # Universeller Fallback f√ºr unbekannte Ger√§tetypen
                return self._decode_generic_protobuf(payload)
                
        except Exception as e:
            _LOGGER.debug(f"Direct protobuf decoding failed for {device_type}: {e}")
            return {"params": {}}
    def _decode_stream_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Stream AC/Ultra/Pro Protobuf-Daten"""
        try:
            from custom_components.ecoflow_cloud.devices.internal.proto import stream_ac_pb2
            
            # Versuche als SendHeaderStreamMsg
            try:
                packet = stream_ac_pb2.SendHeaderStreamMsg()
                packet.ParseFromString(payload)
                
                decoded = {"params": {}}
                
                if hasattr(packet, 'msg') and packet.msg:
                    _LOGGER.debug("SendHeaderStreamMsg.msg found")
                    
                    # Sichere Header-Felder extrahieren
                    for field_name in ['cmd_id', 'src', 'dest', 'seq']:
                        if hasattr(packet.msg, field_name):
                            try:
                                if packet.msg.HasField(field_name):
                                    value = getattr(packet.msg, field_name)
                                    decoded["params"][field_name] = value
                            except Exception:
                                continue
                    
                    # pdata verarbeiten
                    if hasattr(packet.msg, "pdata") and packet.msg.pdata and len(packet.msg.pdata) > 0:
                        pdata = packet.msg.pdata
                        _LOGGER.debug(f"Stream pdata found: {len(pdata)} bytes")
                        
                        # Liste der Stream-Message-Typen
                        stream_types = [
                            ("Champ_cmd21_3", stream_ac_pb2.Champ_cmd21_3),
                            ("Champ_cmd21", stream_ac_pb2.Champ_cmd21), 
                            ("Champ_cmd50", stream_ac_pb2.Champ_cmd50),
                            ("Champ_cmd50_3", stream_ac_pb2.Champ_cmd50_3),
                            ("HeaderStream", stream_ac_pb2.HeaderStream)
                        ]
                        
                        for stream_name, stream_class in stream_types:
                            try:
                                content = stream_class()
                                content.ParseFromString(pdata)
                                
                                content_str = str(content)
                                if len(content_str) > 0 and content_str.strip():
                                    _LOGGER.debug(f"Successfully parsed {stream_name}")
                                    
                                    # Extrahiere alle verf√ºgbaren Felder
                                    for descriptor in content.DESCRIPTOR.fields:
                                        try:
                                            if not content.HasField(descriptor.name):
                                                continue
                                            
                                            value = getattr(content, descriptor.name)
                                            field_name = descriptor.name
                                            
                                            # Parameter verarbeiten
                                            self._process_stream_field(decoded["params"], field_name, value)
                                            
                                        except Exception as field_error:
                                            _LOGGER.debug(f"Failed to process field {descriptor.name}: {field_error}")
                                            continue
                                    
                                    # Wenn wir wichtige Parameter gefunden haben, fr√ºh zur√ºckkehren
                                    if self._has_important_parameters(decoded["params"]):
                                        _LOGGER.info(f"Found key parameters in {stream_name}")
                                        break
                            
                            except Exception as e:
                                _LOGGER.debug(f"Failed to parse {stream_name}: {e}")
                                continue
                
                if decoded["params"]:
                    _LOGGER.info(f"Stream protobuf decoding successful: {len(decoded['params'])} parameters")
                    return decoded
                    
            except Exception as stream_error:
                _LOGGER.debug(f"Stream protobuf decoding failed: {stream_error}")
            
            return {"params": {}}
            
        except ImportError as e:
            _LOGGER.debug(f"Cannot import stream_ac_pb2 for direct decoding: {e}")
            return {"params": {}}
        except Exception as e:
            _LOGGER.debug(f"Stream protobuf decoding completely failed: {e}")
            return {"params": {}}

    def _process_stream_field(self, params: dict, field_name: str, value):
        """Verarbeitet Stream-Felder und f√ºgt sie zu den Parametern hinzu"""
        try:
            # Spezielle Stream-Parameter verarbeiten
            if field_name == "f32ShowSoc":
                params["battery_soc"] = round(value, 2)
                params["f32ShowSoc"] = value
                _LOGGER.info(f"Found Battery SOC: {params['battery_soc']}%")
            elif field_name == "bmsBattSoc":
                params["bms_battery_soc"] = round(value, 2)  
                params["bmsBattSoc"] = value
                _LOGGER.info(f"Found BMS Battery SOC: {params['bms_battery_soc']}%")
            elif field_name == "soc":
                params["soc"] = value
                params["battery_percentage"] = value
                _LOGGER.info(f"Found SOC: {value}%")
            elif field_name == "cycles":
                params["battery_cycles"] = value
                params["cycles"] = value
                _LOGGER.info(f"Found Battery cycles: {value}")
            elif field_name in ["gridConnectionPower", "inputWatts", "outputWatts"]:
                params[field_name] = round(value, 2)
                _LOGGER.info(f"Found {field_name}: {params[field_name]}W")
            else:
                params[field_name] = value
                _LOGGER.debug(f"Found {field_name}: {value}")
        except Exception as e:
            _LOGGER.debug(f"Error processing stream field {field_name}: {e}")

    def _has_important_parameters(self, params: dict) -> bool:
        """Pr√ºft ob wichtige Parameter gefunden wurden"""
        important_keys = ["battery_soc", "soc", "f32ShowSoc", "bmsBattSoc", "gridConnectionPower"]
        return any(key in params for key in important_keys)

    def _decode_delta_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Delta Protobuf-Daten"""
        try:
            # Versuche mit generischen EcoPacket
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            try:
                packet = ecopacket_pb2.Message()
                packet.ParseFromString(payload)
                
                decoded = {"params": {}}
                self._extract_common_packet_fields(packet, decoded["params"])
                
                if decoded["params"]:
                    _LOGGER.info(f"Delta protobuf decoding successful: {len(decoded['params'])} parameters")
                    return decoded
                    
            except Exception as e:
                _LOGGER.debug(f"Delta protobuf parsing failed: {e}")
            
            return {"params": {}}
            
        except ImportError as e:
            _LOGGER.debug(f"Cannot import protobuf modules for Delta: {e}")
            return {"params": {}}

    def _decode_river_protobuf(self, payload: bytes) -> dict:
        """Dekodiert River Protobuf-Daten"""
        try:
            # Versuche mit generischen EcoPacket
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            try:
                packet = ecopacket_pb2.Message()
                packet.ParseFromString(payload)
                
                decoded = {"params": {}}
                self._extract_common_packet_fields(packet, decoded["params"])
                
                if decoded["params"]:
                    _LOGGER.info(f"River protobuf decoding successful: {len(decoded['params'])} parameters")
                    return decoded
                    
            except Exception as e:
                _LOGGER.debug(f"River protobuf parsing failed: {e}")
            
            return {"params": {}}
            
        except ImportError as e:
            _LOGGER.debug(f"Cannot import protobuf modules for River: {e}")
            return {"params": {}}

    def _decode_powerstream_protobuf(self, payload: bytes) -> dict:
        """Dekodiert PowerStream Protobuf-Daten"""
        try:
            # Versuche mit PowerStream-spezifischen Protobuf
            from custom_components.ecoflow_cloud.devices.internal.proto import powerstream_pb2
            
            try:
                packet = powerstream_pb2.PowerStreamMessage()
                packet.ParseFromString(payload)
                
                decoded = {"params": {}}
                self._extract_common_packet_fields(packet, decoded["params"])
                
                if decoded["params"]:
                    _LOGGER.info(f"PowerStream protobuf decoding successful: {len(decoded['params'])} parameters")
                    return decoded
                    
            except Exception as e:
                _LOGGER.debug(f"PowerStream protobuf parsing failed: {e}")
            
            # Fallback zu generischem EcoPacket
            return self._decode_generic_protobuf(payload)
            
        except ImportError as e:
            _LOGGER.debug(f"Cannot import powerstream_pb2: {e}")
            return self._decode_generic_protobuf(payload)

    def _decode_glacier_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Glacier Protobuf-Daten"""
        try:
            # Fallback zu generischem EcoPacket
            return self._decode_generic_protobuf(payload)
        except Exception as e:
            _LOGGER.debug(f"Glacier protobuf decoding failed: {e}")
            return {"params": {}}

    def _decode_wave_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Wave Protobuf-Daten"""
        try:
            # Fallback zu generischem EcoPacket
            return self._decode_generic_protobuf(payload)
        except Exception as e:
            _LOGGER.debug(f"Wave protobuf decoding failed: {e}")
            return {"params": {}}

    def _decode_smart_meter_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Smart Meter Protobuf-Daten"""
        try:
            # Fallback zu generischem EcoPacket
            return self._decode_generic_protobuf(payload)
        except Exception as e:
            _LOGGER.debug(f"Smart Meter protobuf decoding failed: {e}")
            return {"params": {}}

    def _decode_generic_protobuf(self, payload: bytes) -> dict:
        """Universeller Fallback f√ºr alle unbekannten Ger√§tetypen"""
        try:
            # Versuche mit generischen EcoPacket
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            try:
                packet = ecopacket_pb2.Message()
                packet.ParseFromString(payload)
                
                decoded = {"params": {}}
                self._extract_common_packet_fields(packet, decoded["params"])
                
                if decoded["params"]:
                    _LOGGER.info(f"Generic protobuf decoding successful: {len(decoded['params'])} parameters")
                    return decoded
                    
            except Exception as e:
                _LOGGER.debug(f"Generic protobuf parsing failed: {e}")
            
            # Fallback: Hex-Analyse
            return self._analyze_hex_data(payload)
            
        except ImportError as e:
            _LOGGER.debug(f"Cannot import ecopacket_pb2: {e}")
            return self._analyze_hex_data(payload)

    def _extract_common_packet_fields(self, packet, params: dict):
        """Extrahiert gemeinsame Felder aus EcoPacket-Strukturen"""
        try:
            # Standard EcoPacket Felder
            common_fields = ['cmd_func', 'cmd_id', 'device_sn', 'src', 'dest', 'seq']
            
            for field_name in common_fields:
                if hasattr(packet, field_name):
                    try:
                        if packet.HasField(field_name):
                            value = getattr(packet, field_name)
                            params[field_name] = value
                            _LOGGER.debug(f"{field_name}: {value}")
                    except Exception:
                        continue
            
            # Data-Feld verarbeiten wenn vorhanden
            if hasattr(packet, 'data') and packet.HasField('data'):
                data_field = packet.data
                _LOGGER.debug(f"Packet data field found: {len(data_field)} bytes")
                
                # Versuche data-Feld als weitere Protobuf-Struktur zu dekodieren
                # Dies ist ger√§te-spezifisch, daher nur grundlegende Analyse
                params["data_length"] = len(data_field)
                params["data_hex"] = data_field.hex()[:100] + "..." if len(data_field.hex()) > 100 else data_field.hex()
                
        except Exception as e:
            _LOGGER.debug(f"Error extracting common packet fields: {e}")

    def _analyze_hex_data(self, payload: bytes) -> dict:
        """Analysiert Hex-Daten um m√∂gliche Informationen zu extrahieren"""
        try:
            analysis = {
                "params": {
                    "raw_hex": payload.hex()[:200] + "..." if len(payload.hex()) > 200 else payload.hex(),
                    "raw_length": len(payload),
                    "first_bytes": payload[:8].hex() if len(payload) >= 8 else payload.hex(),
                    "last_bytes": payload[-8:].hex() if len(payload) >= 8 else "",
                }
            }
            
            # Suche nach bekannten Patterns
            hex_str = payload.hex()
            
            # Protobuf-typische Patterns
            if "0a" in hex_str[:20]:
                analysis["params"]["protobuf_pattern"] = True
            
            # Versuche Float-Werte zu finden (SOC k√∂nnte als Float kodiert sein)
            import struct
            float_candidates = []
            for i in range(0, len(payload) - 3, 4):
                try:
                    float_val = struct.unpack('<f', payload[i:i+4])[0]
                    if 0 <= float_val <= 100:
                        float_candidates.append((i, float_val))
                except:
                    continue
            
            if float_candidates:
                analysis["params"]["potential_soc_values"] = [f"{val:.2f}" for _, val in float_candidates[:3]]
            
            _LOGGER.debug(f"Hex analysis: {len(payload)} bytes -> {len(analysis['params'])} analysis fields")
            return analysis
            
        except Exception as e:
            _LOGGER.debug(f"Hex analysis failed: {e}")
            return {"params": {"raw_hex": payload.hex() if payload else "", "raw_length": len(payload) if payload else 0}}
        """Dekodiert Stream AC/Ultra/Pro Protobuf-Daten"""
        try:
            # Direkte Stream-Dekodierung mit verbessertem Logging
            decoded = {"params": {}}
            
            try:
                # Versuche direkt mit EcoPacket
                from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
                
                # Versuche als Message (h√§ufigster Stream-Typ)
                packet = ecopacket_pb2.Message()
                packet.ParseFromString(payload)
                
                _LOGGER.info(f"EcoPacket Message parsing successful")
                
                decoded["protobuf_success"] = True
                decoded["message_type"] = "EcoPacketMessage"
                
                # Extrahiere Felder aus Message
                if hasattr(packet, 'cmd_func') and packet.HasField('cmd_func'):
                    decoded["params"]["cmd_func"] = packet.cmd_func
                    _LOGGER.info(f"cmd_func: {packet.cmd_func}")
                
                if hasattr(packet, 'cmd_id') and packet.HasField('cmd_id'):
                    decoded["params"]["cmd_id"] = packet.cmd_id
                    _LOGGER.info(f"cmd_id: {packet.cmd_id}")
                
                if hasattr(packet, 'device_sn') and packet.HasField('device_sn'):
                    decoded["params"]["device_sn"] = packet.device_sn
                    _LOGGER.info(f"device_sn: {packet.device_sn}")
                
                # Wichtig: data Feld verarbeiten
                if hasattr(packet, 'data') and packet.HasField('data'):
                    data_field = packet.data
                    _LOGGER.info(f"EcoPacket data field found: {len(data_field)} bytes")
                    
                    # Versuche data als Stream-spezifischen Content zu dekodieren
                    stream_data = self.decode_stream_data_field(data_field)
                    if stream_data:
                        decoded["params"].update(stream_data)
                        _LOGGER.info(f"Stream data decoded: {len(stream_data)} parameters")
                
                # Wenn wir Parameter gefunden haben, sind wir erfolgreich
                if len(decoded["params"]) > 0:
                    return decoded
                    
            except Exception as packet_error:
                _LOGGER.debug(f"EcoPacket Message parsing failed: {packet_error}")
            
            # Fallback: Versuche mit SendHeaderStreamMsg (ohne problematische Attribute)
            try:
                from custom_components.ecoflow_cloud.devices.internal.proto import stream_ac_pb2
                
                packet = stream_ac_pb2.SendHeaderStreamMsg()
                packet.ParseFromString(payload)
                
                _LOGGER.info(f"SendHeaderStreamMsg parsing successful (fallback)")
                
                decoded["protobuf_success"] = True
                decoded["message_type"] = "SendHeaderStreamMsg"
                
                # Sichere Attribut-Zugriffe ohne problematische Felder
                if hasattr(packet, 'msg') and packet.msg:
                    _LOGGER.info(f"SendHeaderStreamMsg.msg found")
                    
                    # Nur sichere Felder verwenden
                    for field_name in ['cmd_id', 'src', 'dest', 'check_num', 'seq', 'version']:
                        if hasattr(packet.msg, field_name):
                            try:
                                if packet.msg.HasField(field_name):
                                    value = getattr(packet.msg, field_name)
                                    decoded["params"][field_name] = value
                                    _LOGGER.debug(f"{field_name}: {value}")
                            except Exception:
                                continue
                    
                    # Verarbeite pdata wenn vorhanden
                    if hasattr(packet.msg, "pdata") and packet.msg.pdata and len(packet.msg.pdata) > 0:
                        pdata = packet.msg.pdata
                        _LOGGER.info(f"Stream pdata found: {len(pdata)} bytes")
                        
                        # Versuche pdata zu dekodieren
                        pdata_decoded = self.decode_stream_data_field(pdata)
                        if pdata_decoded:
                            decoded["params"].update(pdata_decoded)
                        
                        decoded["params"]["pdata_length"] = len(pdata)
                        decoded["params"]["pdata_hex"] = pdata.hex()[:100] + "..." if len(pdata.hex()) > 100 else pdata.hex()
                
                # Wenn wir Parameter gefunden haben, sind wir erfolgreich
                if len(decoded["params"]) > 0:
                    return decoded
                    
            except Exception as stream_error:
                _LOGGER.debug(f"SendHeaderStreamMsg parsing failed: {stream_error}")
            
            # Fallback: Intelligente Hex-Analyse mit Parameter-Extraktion
            decoded = {
                "protobuf_success": False,
                "message_type": "IntelligentHexAnalysis",
                "params": self.extract_stream_parameters_from_hex(payload)
            }
            
            _LOGGER.info(f"Stream intelligent hex analysis: {len(payload)} bytes -> {len(decoded['params'])} parameters")
            return decoded
            
        except Exception as e:
            _LOGGER.error(f"Stream protobuf decoding completely failed: {e}")
            # Minimaler Fallback
            return {
                "protobuf_success": False,
                "message_type": "Error",
                "error": str(e),
                "params": {
                    "raw_hex": payload.hex() if payload else "",
                    "raw_length": len(payload) if payload else 0
                }
            }

    def decode_stream_data_field(self, data_field: bytes) -> dict:
        """Dekodiert das data-Feld f√ºr Stream-Ger√§te - vollst√§ndige stream_ac.py Logik"""
        try:
            result = {}
            
            if len(data_field) == 0:
                return result
            
            # Komplette Integration der stream_ac.py _parsedata Logik
            from custom_components.ecoflow_cloud.devices.internal.proto import stream_ac_pb2
            
            _LOGGER.debug(f"Parsing Stream data field: {len(data_field)} bytes")
            
            # Liste der Stream-Message-Typen genau wie in stream_ac.py
            stream_types = [
                ("HeaderStream", stream_ac_pb2.HeaderStream),
                ("Champ_cmd21", stream_ac_pb2.Champ_cmd21), 
                ("Champ_cmd21_3", stream_ac_pb2.Champ_cmd21_3),
                ("Champ_cmd50", stream_ac_pb2.Champ_cmd50),
                ("Champ_cmd50_3", stream_ac_pb2.Champ_cmd50_3)
            ]
            
            success_count = 0
            
            for stream_name, stream_class in stream_types:
                try:
                    content = stream_class()
                    content.ParseFromString(data_field)
                    
                    # Pr√ºfe ob wirklicher Inhalt vorhanden ist (wie in Original)
                    content_str = str(content)
                    if len(content_str) > 0 and content_str.strip():
                        _LOGGER.info(f"Successfully parsed {stream_name}")
                        success_count += 1
                        
                        # Extrahiere alle Felder mit genau der gleichen Logik wie stream_ac.py
                        field_count = 0
                        for descriptor in content.DESCRIPTOR.fields:
                            try:
                                if not content.HasField(descriptor.name):
                                    continue
                                
                                value = getattr(content, descriptor.name)
                                field_name = descriptor.name
                                field_count += 1
                                
                                # Exakte Parameter-Verarbeitung wie in stream_ac.py
                                if field_name == "f32ShowSoc":
                                    result["battery_soc"] = round(value, 2)
                                    result["f32ShowSoc"] = value
                                    _LOGGER.info(f"Battery SOC: {result['battery_soc']}%")
                                elif field_name == "bmsBattSoc":
                                    result["bms_battery_soc"] = round(value, 2)  
                                    result["bmsBattSoc"] = value
                                    _LOGGER.info(f"BMS Battery SOC: {result['bms_battery_soc']}%")
                                elif field_name == "soc":
                                    result["soc"] = value
                                    result["battery_percentage"] = value
                                    _LOGGER.info(f"SOC: {value}%")
                                elif field_name in ["bmsChgRemTime", "bmsDsgRemTime"]:
                                    result[field_name] = value
                                    if field_name == "bmsChgRemTime":
                                        result["charge_remaining_minutes"] = value
                                        _LOGGER.info(f"Charge remaining: {value} min")
                                    elif field_name == "bmsDsgRemTime":
                                        result["discharge_remaining_minutes"] = value
                                        _LOGGER.info(f"Discharge remaining: {value} min")
                                elif field_name == "cycles":
                                    result["battery_cycles"] = value
                                    result["cycles"] = value
                                    _LOGGER.info(f"Battery cycles: {value}")
                                elif field_name in ["designCap", "fullCap", "remainCap"]:
                                    result[field_name] = value
                                    _LOGGER.info(f"{field_name}: {value}")
                                elif field_name in ["gridConnectionPower", "inputWatts", "outputWatts"]:
                                    result[field_name] = round(value, 2)
                                    _LOGGER.info(f"{field_name}: {result[field_name]}W")
                                elif field_name in ["powGetPvSum", "powGetBpCms", "powGetSysGrid"]:
                                    result[field_name] = round(value, 2)
                                    _LOGGER.info(f"{field_name}: {result[field_name]}W")
                                elif field_name in ["maxCellTemp", "minCellTemp", "temp"]:
                                    result[field_name] = value
                                    _LOGGER.debug(f"{field_name}: {value}¬∞C")
                                elif field_name in ["maxCellVol", "minCellVol", "vol"]:
                                    result[field_name] = value
                                    _LOGGER.debug(f"{field_name}: {value}V")
                                else:
                                    result[field_name] = value
                                    _LOGGER.debug(f"{field_name}: {value}")
                                    
                            except Exception as field_error:
                                _LOGGER.debug(f"Failed to process field {descriptor.name}: {field_error}")
                                continue
                        
                        _LOGGER.info(f"{stream_name}: extracted {field_count} fields")
                        
                        # Wenn wir wichtige Parameter gefunden haben, k√∂nnen wir fr√ºh zur√ºckkehren
                        if any(key in result for key in ["battery_soc", "soc", "battery_percentage"]):
                            _LOGGER.info(f"Found key battery parameters in {stream_name}")
                            break
                
                except Exception as e:
                    _LOGGER.debug(f"Failed to parse {stream_name}: {e}")
                    continue
            
            if success_count > 0:
                _LOGGER.info(f"Stream data parsing successful: {success_count} message types parsed, {len(result)} total parameters")
                
                # Log wichtige gefundene Parameter
                key_params = []
                if "battery_soc" in result:
                    key_params.append(f"battery_soc={result['battery_soc']}%")
                if "soc" in result:
                    key_params.append(f"soc={result['soc']}%")
                if "battery_cycles" in result:
                    key_params.append(f"cycles={result['battery_cycles']}")
                
                power_params = [k for k in result.keys() if 'power' in k.lower() or 'watt' in k.lower()]
                if power_params:
                    key_params.append(f"power_fields={len(power_params)}")
                
                if key_params:
                    _LOGGER.info(f"Key parameters: {', '.join(key_params)}")
                
                return result
            else:
                _LOGGER.debug(f"No Stream message types successfully parsed from {len(data_field)} bytes")
                return {}
                
        except ImportError as e:
            _LOGGER.error(f"Cannot import stream_ac_pb2: {e}")
            return {}
        except Exception as e:
            _LOGGER.debug(f"Stream data field decoding failed: {e}")
            return {}

    def extract_stream_parameters_from_hex(self, payload: bytes) -> dict:
        """Extrahiert Stream-Parameter durch intelligente Hex-Analyse"""
        try:
            result = {}
            
            if len(payload) < 4:
                return {"raw_hex": payload.hex(), "raw_length": len(payload)}
            
            # Analysiere Protobuf-Struktur
            hex_str = payload.hex()
            result["raw_hex"] = hex_str[:100] + "..." if len(hex_str) > 100 else hex_str
            result["raw_length"] = len(payload)
            
            # Suche nach typischen Stream-Parametern in verschiedenen Datenformaten
            import struct
            
            # Float-Werte suchen (32-bit IEEE 754)
            float_candidates = []
            for i in range(len(payload) - 3):
                try:
                    float_val = struct.unpack('<f', payload[i:i+4])[0]  # Little-endian
                    if 0 <= float_val <= 200 and float_val != 0:  # Plausible Werte
                        float_candidates.append((i, float_val))
                except:
                    continue
            
            # Integer-Werte suchen (16-bit und 32-bit)
            int16_candidates = []
            int32_candidates = []
            
            for i in range(len(payload) - 1):
                try:
                    val16 = struct.unpack('<H', payload[i:i+2])[0]  # Unsigned 16-bit
                    if 0 <= val16 <= 65000:
                        int16_candidates.append((i, val16))
                except:
                    continue
            
            for i in range(len(payload) - 3):
                try:
                    val32 = struct.unpack('<I', payload[i:i+4])[0]  # Unsigned 32-bit
                    if 0 <= val32 <= 1000000:
                        int32_candidates.append((i, val32))
                except:
                    continue
            
            # Interpretiere die besten Kandidaten
            param_count = 0
            
            # Float-Werte als potentielle SOC/Power-Werte
            for offset, value in float_candidates[:5]:  # Top 5
                if 0 < value <= 100:
                    result[f"float_param_{param_count}_soc_candidate"] = round(value, 2)
                    param_count += 1
                elif 0 < value <= 10000:
                    result[f"float_param_{param_count}_power_candidate"] = round(value, 1)
                    param_count += 1
            
            # Integer-Werte als potentielle Status/Counter
            for offset, value in int16_candidates[:3]:  # Top 3
                if 0 < value <= 1000:
                    result[f"int16_param_{param_count}"] = value
                    param_count += 1
            
            for offset, value in int32_candidates[:2]:  # Top 2
                if 0 < value <= 100000:
                    result[f"int32_param_{param_count}"] = value
                    param_count += 1
            
            # Spezielle Stream Ultra Patterns
            if len(payload) >= 20:
                # Suche nach typischen Stream-Sequenzen
                if b'\x08\x10' in payload or b'\x10\x18' in payload:
                    result["stream_pattern_detected"] = True
                
                # Zeitstempel-√§hnliche Patterns
                for i in range(len(payload) - 7):
                    try:
                        timestamp_candidate = struct.unpack('<Q', payload[i:i+8])[0]
                        if 1600000000 <= timestamp_candidate <= 2000000000:  # Plausible Unix-Timestamps
                            result["timestamp_candidate"] = timestamp_candidate
                            break
                    except:
                        continue
            
            _LOGGER.debug(f"Extracted {param_count} parameters from hex analysis")
            return result
            
        except Exception as e:
            _LOGGER.debug(f"Hex parameter extraction failed: {e}")
            return {"raw_hex": payload.hex() if payload else "", "raw_length": len(payload) if payload else 0}

    def _process_stream_field_safe(self, result: dict, field_name: str, value):
        """Verarbeitet Stream-Felder sicher ohne Exceptions"""
        try:
            # Wichtige Stream-Parameter mit bekannten Namen
            if field_name in ["soc", "battery_soc", "f32ShowSoc", "bmsBattSoc"]:
                if isinstance(value, (int, float)) and 0 <= value <= 100:
                    result["battery_soc"] = round(float(value), 2)
                    result[field_name] = value
            elif field_name in ["watts", "power", "gridPower", "ac_power", "dc_power"]:
                if isinstance(value, (int, float)):
                    result["power_watts"] = round(float(value), 1)
                    result[field_name] = value
            elif field_name in ["voltage", "vol", "battery_voltage"]:
                if isinstance(value, (int, float)) and 0 <= value <= 1000:
                    result["voltage"] = round(float(value), 2)
                    result[field_name] = value
            elif field_name in ["current", "amp", "battery_current"]:
                if isinstance(value, (int, float)):
                    result["current"] = round(float(value), 2)
                    result[field_name] = value
            elif field_name in ["temp", "temperature", "battery_temp"]:
                if isinstance(value, (int, float)) and -50 <= value <= 100:
                    result["temperature"] = round(float(value), 1)
                    result[field_name] = value
            elif field_name in ["cycles", "charge_cycles"]:
                if isinstance(value, int) and 0 <= value <= 10000:
                    result["battery_cycles"] = value
                    result[field_name] = value
            else:
                # Generische Behandlung
                converted_value = self.convert_protobuf_value(value)
                if converted_value is not None:
                    result[field_name] = converted_value
                    
        except Exception as e:
            _LOGGER.debug(f"Error processing stream field {field_name}: {e}")
            # Fallback: String-Konvertierung
            try:
                result[field_name] = str(value)
            except:
                pass

    def _analyze_hex_data(self, payload: bytes) -> dict:
        """Analysiert Hex-Daten um m√∂gliche Informationen zu extrahieren"""
        try:
            analysis = {
                "length": len(payload),
                "first_bytes": payload[:8].hex() if len(payload) >= 8 else payload.hex(),
                "last_bytes": payload[-8:].hex() if len(payload) >= 8 else "",
            }
            
            # Suche nach bekannten Patterns
            hex_str = payload.hex()
            
            # Stream-typische Patterns
            if "0a" in hex_str[:20]:  # Protobuf field markers
                analysis["protobuf_detected"] = True
            
            # Versuche Float-Werte zu finden (SOC k√∂nnte als Float kodiert sein)
            if len(payload) >= 4:
                import struct
                for i in range(len(payload) - 3):
                    try:
                        float_val = struct.unpack('f', payload[i:i+4])[0]
                        if 0 <= float_val <= 100:  # M√∂glicher SOC-Wert
                            analysis[f"possible_soc_at_offset_{i}"] = round(float_val, 2)
                    except:
                        continue
            
            return analysis
            
        except Exception as e:
            return {"error": str(e)}

    def decode_delta2_protobuf(self, payload: bytes) -> dict:
        """Dekodiert Delta 2 Protobuf-Daten"""
        try:
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            packet = ecopacket_pb2.Header()
            packet.ParseFromString(payload)
            
            decoded = {"params": {}, "protobuf_success": True}
            for field in packet.DESCRIPTOR.fields:
                if packet.HasField(field.name):
                    value = getattr(packet, field.name)
                    decoded["params"][field.name] = self.convert_protobuf_value(value)
            
            return decoded
            
        except Exception as e:
            _LOGGER.debug(f"Delta2 protobuf decoding failed: {e}")
            return self.decode_generic_protobuf(payload)

    def decode_powerstream_protobuf(self, payload: bytes) -> dict:
        """Dekodiert PowerStream Protobuf-Daten"""
        try:
            from custom_components.ecoflow_cloud.devices.internal.proto import powerstream_pb2
            
            # Versuche PowerStream-spezifische Nachrichten
            decoded = {"params": {}, "protobuf_success": True}
            
            # Hier k√∂nnten PowerStream-spezifische Nachrichten-Typen hinzugef√ºgt werden
            # Fallback auf generische Dekodierung
            decoded.update(self.decode_generic_protobuf(payload))
            
            return decoded
            
        except Exception as e:
            _LOGGER.debug(f"PowerStream protobuf decoding failed: {e}")
            return self.decode_generic_protobuf(payload)

    def decode_river_protobuf(self, payload: bytes) -> dict:
        """Dekodiert River Protobuf-Daten"""
        try:
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            packet = ecopacket_pb2.Header()
            packet.ParseFromString(payload)
            
            decoded = {"params": {}, "protobuf_success": True}
            for field in packet.DESCRIPTOR.fields:
                if packet.HasField(field.name):
                    value = getattr(packet, field.name)
                    decoded["params"][field.name] = self.convert_protobuf_value(value)
            
            return decoded
            
        except Exception as e:
            _LOGGER.debug(f"River protobuf decoding failed: {e}")
            return self.decode_generic_protobuf(payload)

    def decode_generic_protobuf(self, payload: bytes) -> dict:
        """Generische Protobuf-Dekodierung f√ºr alle Ger√§te"""
        try:
            from custom_components.ecoflow_cloud.devices.internal.proto import ecopacket_pb2
            
            decoded = {"params": {}}
            
            # Versuche als Header
            try:
                packet = ecopacket_pb2.Header()
                packet.ParseFromString(payload)
                
                decoded["protobuf_success"] = True
                decoded["message_type"] = "Header"
                
                for field in packet.DESCRIPTOR.fields:
                    if packet.HasField(field.name):
                        value = getattr(packet, field.name)
                        key = field.name
                        
                        # Spezielle Behandlung f√ºr wichtige Felder
                        if key == "pdata" and isinstance(value, bytes):
                            # Versuche pdata zu dekodieren
                            pdata_decoded = self.decode_pdata_content(value)
                            if pdata_decoded:
                                decoded["params"].update(pdata_decoded)
                            decoded["params"]["pdata_hex"] = value.hex()
                            decoded["params"]["pdata_length"] = len(value)
                        else:
                            decoded["params"][key] = self.convert_protobuf_value(value)
                
                return decoded
                
            except Exception:
                pass
            
            # Versuche als SendMsgHart
            try:
                msg_hart = ecopacket_pb2.SendMsgHart()
                msg_hart.ParseFromString(payload)
                
                decoded["protobuf_success"] = True
                decoded["message_type"] = "SendMsgHart"
                
                for field in msg_hart.DESCRIPTOR.fields:
                    if msg_hart.HasField(field.name):
                        value = getattr(msg_hart, field.name)
                        decoded["params"][field.name] = self.convert_protobuf_value(value)
                
                return decoded
                
            except Exception:
                pass
            
            # Wenn alle Versuche fehlschlagen
            decoded["protobuf_success"] = False
            decoded["message_type"] = "Unknown"
            return decoded
            
        except Exception as e:
            _LOGGER.debug(f"Generic protobuf decoding failed: {e}")
            return {"protobuf_success": False, "params": {}}

    def decode_pdata_content(self, pdata: bytes) -> dict:
        """Versucht den Inhalt von pdata zu dekodieren"""
        try:
            # Versuche verschiedene bekannte Strukturen
            result = {}
            
            # Einfache Hex-Analyse f√ºr bekannte Patterns
            hex_data = pdata.hex()
            
            # Stream-spezifische Patterns
            if len(pdata) > 50:  # Stream-Daten sind typischerweise l√§nger
                # Einfache Hex-Analyse statt problematischer Protobuf-Dekodierung
                result.update(self._analyze_hex_data(pdata))
            
            return result
            
        except Exception as e:
            _LOGGER.debug(f"pdata decoding failed: {e}")
            return {}

    def convert_protobuf_value(self, value):
        """Konvertiert Protobuf-Werte zu JSON-kompatiblen Typen"""
        if isinstance(value, bytes):
            # Versuche als UTF-8 String, sonst als Hex
            try:
                decoded_str = value.decode('utf-8')
                # Pr√ºfe ob es sinnvoller Text ist
                if all(ord(c) < 128 and (c.isprintable() or c.isspace()) for c in decoded_str):
                    return decoded_str
                else:
                    return value.hex()
            except:
                return value.hex()
        elif hasattr(value, 'DESCRIPTOR'):  # Nested message
            result = {}
            for field in value.DESCRIPTOR.fields:
                if value.HasField(field.name):
                    field_value = getattr(value, field.name)
                    result[field.name] = self.convert_protobuf_value(field_value)
            return result
        elif isinstance(value, (list, tuple)):
            return [self.convert_protobuf_value(item) for item in value]
        elif isinstance(value, float):
            # Runde Float-Werte f√ºr bessere Lesbarkeit
            return round(value, 3)
        else:
            return value

    async def send_keep_alive_messages(self):
        """
        Erweiterte Keep-Alive Implementierung basierend auf ioBroker.ecoflow-mqtt
        Sendet regelm√§√üige Quota-Anfragen und √ºberwacht Verbindungsstatus
        """
        if not self.api_client:
            _LOGGER.warning("API Client not available for keep-alive")
            return
            
        try:
            keep_alive_success = False
            
            # 1. Quota-Anfragen f√ºr alle Ger√§te (wie ioBroker Implementation)
            try:
                if hasattr(self.api_client, 'quota_all'):
                    await self.api_client.quota_all(None)
                    keep_alive_success = True
                    _LOGGER.debug("quota_all Keep-Alive sent")
                    
                # Zus√§tzlich: Einzelne Ger√§te-Quotas abrufen
                for device_sn in self.device_sns:
                    try:
                        # Protobuf Quota anfragen (f√ºr PowerStream, Delta, River etc.)
                        if hasattr(self.api_client, 'get_device_quota'):
                            await self.api_client.get_device_quota(device_sn)
                        
                        # JSON Quota anfragen (f√ºr andere Ger√§te)  
                        if hasattr(self.api_client, 'get_device_info'):
                            await self.api_client.get_device_info(device_sn)
                            
                    except Exception as device_error:
                        _LOGGER.debug(f"Device-specific quota failed for {device_sn}: {device_error}")
                        continue
                        
            except Exception as quota_error:
                _LOGGER.debug(f"Quota keep-alive failed: {quota_error}")
            
            # 2. MQTT-Level Keep-Alive simulieren (wie echte App)
            try:
                if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
                    # Sende Ping-√§hnliche Nachrichten f√ºr jedes Ger√§t
                    for device_sn in self.device_sns:
                        try:
                            # Simuliere App-Keep-Alive mit heartbeat message
                            heartbeat_topic = f"/app/{self.username}/{device_sn}/thing/property/heartbeat"
                            heartbeat_payload = {
                                "id": 40,  # Heartbeat ID (wie in ioBroker gesehen)
                                "version": "1.0",
                                "timestamp": int(time.time() * 1000)
                            }
                            
                            # Pr√ºfe ob MQTT Client verf√ºgbar
                            mqtt_client = None
                            if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                                mqtt_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
                            elif hasattr(self.api_client.mqtt_client, '__client'):
                                mqtt_client = self.api_client.mqtt_client.__client
                            
                            if mqtt_client and mqtt_client.is_connected():
                                mqtt_client.publish(
                                    heartbeat_topic, 
                                    json.dumps(heartbeat_payload),
                                    qos=1
                                )
                                keep_alive_success = True
                                _LOGGER.debug(f"Heartbeat sent for {device_sn}")
                            else:
                                _LOGGER.warning(f"MQTT not connected for heartbeat to {device_sn}")
                                
                        except Exception as heartbeat_error:
                            _LOGGER.debug(f"Heartbeat failed for {device_sn}: {heartbeat_error}")
                            continue
                            
            except Exception as mqtt_error:
                _LOGGER.debug(f"MQTT keep-alive failed: {mqtt_error}")
            
            # 3. Verbindungsstatus pr√ºfen und bei Bedarf reconnect
            try:
                if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
                    mqtt_client = None
                    if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                        mqtt_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
                    elif hasattr(self.api_client.mqtt_client, '__client'):
                        mqtt_client = self.api_client.mqtt_client.__client
                    
                    if mqtt_client:
                        if not mqtt_client.is_connected():
                            _LOGGER.warning("MQTT connection lost, attempting reconnect")
                            try:
                                mqtt_client.reconnect()
                                keep_alive_success = True
                            except Exception as reconnect_error:
                                _LOGGER.error(f"MQTT reconnect failed: {reconnect_error}")
                        else:
                            _LOGGER.debug("MQTT connection healthy")
                            
            except Exception as connection_error:
                _LOGGER.debug(f"Connection check failed: {connection_error}")
            
            # Status-Log
            if keep_alive_success:
                _LOGGER.info(f"Keep-Alive successful for {len(self.device_sns)} devices")
            else:
                _LOGGER.warning(f"Keep-Alive failed - no successful method")
                
        except Exception as e:
            _LOGGER.error(f"Keep-Alive system error: {e}")
            _LOGGER.error(f"Full traceback: {traceback.format_exc()}")

    async def send_heartbeat_messages(self):
        """
        Sendet regelm√§√üige Heartbeat-Nachrichten (simuliert App-Aktivit√§t)
        Basierend auf ioBroker implementation - h√§ufigere, kleinere Keep-Alive Signale
        """
        if not self.api_client:
            return
            
        try:
            if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
                # MQTT Client finden
                mqtt_client = None
                if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                    mqtt_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
                elif hasattr(self.api_client.mqtt_client, '__client'):
                    mqtt_client = self.api_client.mqtt_client.__client
                
                if mqtt_client and mqtt_client.is_connected():
                    # F√ºr jedes Ger√§t Heartbeat senden
                    for device_sn in self.device_sns:
                        try:
                            # Heartbeat message wie in ioBroker (JSON mit ID 40/68/72)
                            heartbeat_topic = f"/app/{self.username}/{device_sn}/thing/property/set"
                            heartbeat_payload = {
                                "id": 40,  # Heartbeat ID wie in ioBroker
                                "version": "1.0",
                                "params": {},
                                "timestamp": int(time.time() * 1000)
                            }
                            
                            mqtt_client.publish(
                                heartbeat_topic, 
                                json.dumps(heartbeat_payload),
                                qos=0  # QoS 0 f√ºr h√§ufige Heartbeats
                            )
                            
                            _LOGGER.debug(f"Heartbeat sent for {device_sn}")
                            
                        except Exception as device_error:
                            _LOGGER.debug(f"Heartbeat failed for {device_sn}: {device_error}")
                            continue
                            
                else:
                    _LOGGER.warning("MQTT not connected for heartbeat")
                    
        except Exception as e:
            _LOGGER.debug(f"Heartbeat system error: {e}")

    async def start(self):
        """Startet den EcoFlow MQTT Publisher"""
        _LOGGER.info("EcoFlow MQTT Publisher starting...")
        
        try:
            # MQTT Client setup
            self.setup_mqtt_client()
            
            # EcoFlow API Client setup
            await self.setup_api_client()
            
            # Message Forwarding setup
            self.setup_ecoflow_message_forwarding()
            
            # Kurz warten f√ºr die EcoFlow MQTT-Verbindung
            _LOGGER.info("‚è≥ Waiting for EcoFlow MQTT connection...")
            await asyncio.sleep(5)
            
            # Verbindungsstatus nochmals pr√ºfen
            if hasattr(self.api_client, 'mqtt_client') and self.api_client.mqtt_client:
                ecoflow_paho_client = None
                if hasattr(self.api_client.mqtt_client, '_EcoflowMQTTClient__client'):
                    ecoflow_paho_client = self.api_client.mqtt_client._EcoflowMQTTClient__client
                elif hasattr(self.api_client.mqtt_client, '__client'):
                    ecoflow_paho_client = self.api_client.mqtt_client.__client
                
                if ecoflow_paho_client:
                    connected = ecoflow_paho_client.is_connected()
                    _LOGGER.info(f"EcoFlow MQTT final status: Connected={connected}")
                    if not connected:
                        _LOGGER.warning("EcoFlow MQTT not yet connected - messages may arrive delayed")
                    else:
                        _LOGGER.info("EcoFlow MQTT fully connected and ready")
            
            # Nachrichten-Z√§hler f√ºr Debugging
            self.message_count = 0
            self.last_message_time = None
            
            # Keep-Alive System Konfiguration (wie ioBroker optimiert)
            self.keep_alive_interval = 300   # Quota-Abfragen alle 5 Minuten (wie ioBroker)
            self.heartbeat_interval = 60     # Heartbeat alle 60 Sekunden 
            self.status_interval = 60        # Status-Updates alle 60 Sekunden
            self.last_keep_alive = 0
            self.last_heartbeat = 0
            self.last_status_update = 0
            
            # Hauptschleife starten
            self.running = True
            status_counter = 0
            keep_alive_counter = 0
            heartbeat_counter = 0
            
            while self.running:
                current_time = time.time()
                
                # Keep-Alive Quota-Anfragen (alle 5 Minuten wie ioBroker)
                if current_time - self.last_keep_alive >= self.keep_alive_interval:
                    keep_alive_counter += 1
                    await self.send_keep_alive_messages()
                    self.last_keep_alive = current_time
                    
                    msg_info = f"Messages so far: {getattr(self, 'message_count', 0)}"
                    if getattr(self, 'last_message_time', None):
                        seconds_ago = int(time.time() - self.last_message_time)
                        msg_info += f", last {seconds_ago}s ago"
                    else:
                        msg_info += ", none received yet"
                    
                    _LOGGER.info(f"Keep-Alive #{keep_alive_counter} sent to {len(self.device_sns)} devices - {msg_info}")
                
                # Heartbeat-Nachrichten (alle 60 Sekunden)
                if current_time - self.last_heartbeat >= self.heartbeat_interval:
                    heartbeat_counter += 1
                    await self.send_heartbeat_messages()
                    self.last_heartbeat = current_time
                    _LOGGER.debug(f"Heartbeat #{heartbeat_counter} sent to {len(self.device_sns)} devices")
                
                # Status-Updates senden (alle 60s)
                if current_time - self.last_status_update >= self.status_interval:
                    status_counter += 1
                    
                    # Status-Update senden
                    message_stats = {
                        'total_messages': getattr(self, 'message_count', 0),
                        'last_message_time': getattr(self, 'last_message_time', None),
                        'seconds_since_last_message': time.time() - getattr(self, 'last_message_time', time.time()) if getattr(self, 'last_message_time', None) else None,
                        'keep_alive_count': keep_alive_counter
                    }
                    
                    status = {
                        'status': 'running',
                        'timestamp': time.time(),
                        'devices': self.device_sns,
                        'messages': message_stats,
                        'status_update_count': status_counter,
                        'keep_alive_enabled': True,
                        'keep_alive_interval': self.keep_alive_interval
                    }
                    
                    self.mqtt_client.publish(
                        f"{self.mqtt_base_topic}/status", 
                        json.dumps(status), 
                        retain=True
                    )
                    
                    # Status f√ºr jedes Ger√§t senden
                    for device_sn in self.device_sns:
                        device_status = {
                            'device_sn': device_sn,
                            'timestamp': time.time(),
                            'status': 'connected',
                            'keep_alive_active': True
                        }
                        topic = f"{self.mqtt_base_topic}/{device_sn}/status"
                        self.mqtt_client.publish(topic, json.dumps(device_status), retain=True)
                    
                    self.last_status_update = current_time
                    _LOGGER.info(f"Status update #{status_counter} sent for {len(self.device_sns)} devices")
                
                # Kurze Pause zwischen Zyklen
                await asyncio.sleep(5)  # Schnellere Kontrolle f√ºr pr√§ziseres Timing
                
        except Exception as e:
            _LOGGER.error(f"Error during startup: {e}")
            raise

    def stop(self):
        """Stoppt den Publisher"""
        _LOGGER.info("EcoFlow MQTT Publisher stopping...")
        self.running = False
        
        # Message timeout timer stoppen
        if hasattr(self, 'message_timeout_timer') and self.message_timeout_timer:
            self.message_timeout_timer.cancel()
            _LOGGER.info("Message timeout timer stopped")
        
        if self.api_client:
            self.api_client.stop()
            
        if self.mqtt_client:
            self.mqtt_client.loop_stop()
            self.mqtt_client.disconnect()


def signal_handler(signum, frame):
    """Signal Handler f√ºr graceful shutdown"""
    _LOGGER.info(f"Signal {signum} received, shutting down...")
    if hasattr(signal_handler, 'publisher'):
        signal_handler.publisher.stop()
    sys.exit(0)


async def main():
    """Hauptfunktion"""
    # Signal Handler registrieren
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        publisher = EcoflowMqttPublisher()
        signal_handler.publisher = publisher  # F√ºr Signal Handler verf√ºgbar machen
        await publisher.start()
    except Exception as e:
        _LOGGER.error(f"Error during startup: {e}")
        _LOGGER.error("Full traceback:", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())
